{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Facebook Scraping\n",
        "\n",
        "This code segment is a Python function designed to scrape data from a Facebook page using BeautifulSoup and Selenium libraries. Here's a breakdown:\n",
        "\n",
        "1. Import necessary libraries:\n",
        "    ```python\n",
        "    from bs4 import BeautifulSoup\n",
        "    from selenium import webdriver\n",
        "    from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
        "    from selenium.webdriver.common.by import By\n",
        "    import time\n",
        "    import pandas as pd\n",
        "    ```\n",
        "\n",
        "2. Define the function `get_facebook_post_data`:\n",
        "    - Parameters:\n",
        "        - `page_url`: URL of the Facebook page to scrape.\n",
        "        - `scroll_count` (optional): Number of times to scroll down to load more posts.\n",
        "    - Returns: A DataFrame containing extracted post data.\n",
        "\n",
        "3. Inside the function:\n",
        "    - It initializes a Chrome WebDriver assuming the path to `chromedriver` is valid.\n",
        "    - Navigates to the provided Facebook page URL.\n",
        "    - Clicks the \"See More Posts\" button if present.\n",
        "    - Scrolls down to load more posts based on the `scroll_count`.\n",
        "    - Uses BeautifulSoup to parse the page source.\n",
        "    - Extracts post data (title, link, date, reactions, comments) using specified class names.\n",
        "    - Constructs a DataFrame from the extracted data and returns it.\n",
        "    - Catches `NoSuchElementException` and `TimeoutException` errors and prints them.\n",
        "\n",
        "4. Usage example:\n",
        "    ```python\n",
        "    series = get_facebook_post_data('https://web.facebook.com/alwa3d4', scroll_count=80)\n",
        "    ```\n",
        "\n",
        "Note: Ensure you have the required Chrome WebDriver (`chromedriver`) installed and available at the specified path before executing this code.\n"
      ],
      "metadata": {
        "id": "cdtnrH7SYs5c"
      },
      "id": "cdtnrH7SYs5c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbb6a636",
      "metadata": {
        "id": "cbb6a636"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
        "from selenium.webdriver.common.by import By  # Use By for cleaner selectors\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def get_facebook_post_data(page_url, scroll_count=2):\n",
        "    \"\"\"\n",
        "    Extracts post data (title, link, reactions, comments) from a Facebook page.\n",
        "    Args:\n",
        "        page_url (str): The URL of the Facebook page.\n",
        "        scroll_count (int, optional): The number of times to scroll down to load more posts. Defaults to 2.\n",
        "    Returns:\n",
        "        pandas.DataFrame: A DataFrame containing the extracted post data, or an empty DataFrame\n",
        "            if an error occurs.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        driver = webdriver.Chrome('C:/Users/sejja/chromedriver')  # Assuming valid path\n",
        "\n",
        "        driver.get(page_url)\n",
        "        time.sleep(2)  # Adjust sleep time as needed\n",
        "\n",
        "        # Click the \"See More Posts\" button if present (using By for reliability)\n",
        "        try:\n",
        "            button = driver.find_element(By.CLASS_NAME, \"x1tk7jg1\")  # Class name of the button\n",
        "            button.click()\n",
        "        except NoSuchElementException:\n",
        "            pass  # Ignore if button not found\n",
        "\n",
        "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        for _ in range(scroll_count):\n",
        "            driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
        "            time.sleep(2)  # Adjust sleep time as needed\n",
        "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "            if new_height == last_height:\n",
        "                break\n",
        "            last_height = new_height\n",
        "\n",
        "        src = driver.page_source\n",
        "        soup = BeautifulSoup(src, 'lxml')\n",
        "\n",
        "        data = []\n",
        "        for post in soup.find_all('div', {'class': 'x1yztbdb x1n2onr6 xh8yej3 x1ja2u2z'}):\n",
        "            try:\n",
        "                title_element = post.find('div', {'class': 'xdj266r x11i5rnm xat24cr x1mh8g0r x1vvkbs x126k92a'})\n",
        "                link_element = post.find('a', {'class': 'x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz x1heor9g xt0b8zv xo1l8bm'})\n",
        "                react_element = post.find('span', {'class': 'xrbpyxo x6ikm8r x10wlt62 xlyipyv x1exxlbk'})\n",
        "                comments_element = post.find('span', {'class': 'x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xi81zsa'})\n",
        "\n",
        "                if title_element and link_element and react_element and comments_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    link = link_element.get('href')\n",
        "                    date = link_element.text.strip()\n",
        "                    react = react_element.text.strip()\n",
        "                    comments = comments_element.text.strip()\n",
        "                    data.append({'Title': title, 'Link': link, 'React': react,'Date':date, 'Comments': comments})\n",
        "            except AttributeError:\n",
        "                pass  # Skip posts with missing elements\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        return df\n",
        "\n",
        "    except (NoSuchElementException, TimeoutException) as e:\n",
        "        print(e)\n",
        "#series=get_facebook_post_data('https://web.facebook.com/alwa3d4', scroll_count=80)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YKL7m3F_Yr0d"
      },
      "id": "YKL7m3F_Yr0d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "525b74cb",
      "metadata": {
        "id": "525b74cb",
        "outputId": "e0cd6b75-002e-4c34-ad6c-35e4d1eb9f88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'4,3\\xa0K'"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sorting Posts by Number of Reactions\n",
        "\n",
        "This code segment demonstrates how to sort posts by the number of reactions. Here's a step-by-step explanation:"
      ],
      "metadata": {
        "id": "f3YkEHFKZgrr"
      },
      "id": "f3YkEHFKZgrr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f506da24",
      "metadata": {
        "id": "f506da24",
        "outputId": "7d66a8f8-2c03-4ee4-98de-6fc0ee5508b1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Title</th>\n",
              "      <th>Link</th>\n",
              "      <th>React</th>\n",
              "      <th>Date</th>\n",
              "      <th>Comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>79</td>\n",
              "      <td>العاىلة الكاطورزية</td>\n",
              "      <td>https://web.facebook.com/alwa3d4/posts/pfbid02...</td>\n",
              "      <td>9900</td>\n",
              "      <td>4 j</td>\n",
              "      <td>834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>68</td>\n",
              "      <td>العين خيبة ميمكنش يكنو تفاقو</td>\n",
              "      <td>https://web.facebook.com/alwa3d4/posts/pfbid0x...</td>\n",
              "      <td>9500</td>\n",
              "      <td>4 j</td>\n",
              "      <td>2,3 K</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>104</td>\n",
              "      <td>الاحداث القادمة غادي تشلل</td>\n",
              "      <td>https://web.facebook.com/alwa3d4/posts/pfbid0r...</td>\n",
              "      <td>9300</td>\n",
              "      <td>21 mars à 02:14</td>\n",
              "      <td>688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>169</td>\n",
              "      <td>جابها تخدم معاه بلا مايعرف أنها بنتأكثر وحدين ...</td>\n",
              "      <td>https://web.facebook.com/alwa3d4/posts/pfbid03...</td>\n",
              "      <td>9200</td>\n",
              "      <td>17 mars à 20:13</td>\n",
              "      <td>678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>-هاذ الطيحات ماااشي ديال المغاربة، بشاااخ فين ...</td>\n",
              "      <td>https://web.facebook.com/alwa3d4/posts/pfbid02...</td>\n",
              "      <td>8900</td>\n",
              "      <td>1 j</td>\n",
              "      <td>546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>92</td>\n",
              "      <td>تخيلو معايا يكون هدا هو الأب الحقيقي لحمزةصافي...</td>\n",
              "      <td>https://web.facebook.com/alwa3d4/posts/pfbid02...</td>\n",
              "      <td>73</td>\n",
              "      <td>24 mars à 04:07</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>93</td>\n",
              "      <td>ناري قتلني بضحك</td>\n",
              "      <td>https://web.facebook.com/alwa3d4/posts/pfbid0z...</td>\n",
              "      <td>63</td>\n",
              "      <td>24 mars à 03:46</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>89</td>\n",
              "      <td>حتى هاد الكي خطار</td>\n",
              "      <td>https://web.facebook.com/alwa3d4/posts/pfbid03...</td>\n",
              "      <td>43</td>\n",
              "      <td>6 j</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>90</td>\n",
              "      <td>قلتلها يمكن</td>\n",
              "      <td>https://web.facebook.com/alwa3d4/posts/pfbid02...</td>\n",
              "      <td>29</td>\n",
              "      <td>6 j</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>66</td>\n",
              "      <td>النيابة العامة الإسبانية تطالب بسجن مدرب المنت...</td>\n",
              "      <td>https://web.facebook.com/alwa3d4/posts/pfbid02...</td>\n",
              "      <td>28</td>\n",
              "      <td>3 j</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>262 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0                                              Title  \\\n",
              "79           79                                 العاىلة الكاطورزية   \n",
              "68           68                       العين خيبة ميمكنش يكنو تفاقو   \n",
              "104         104                          الاحداث القادمة غادي تشلل   \n",
              "169         169  جابها تخدم معاه بلا مايعرف أنها بنتأكثر وحدين ...   \n",
              "25           25  -هاذ الطيحات ماااشي ديال المغاربة، بشاااخ فين ...   \n",
              "..          ...                                                ...   \n",
              "92           92  تخيلو معايا يكون هدا هو الأب الحقيقي لحمزةصافي...   \n",
              "93           93                                    ناري قتلني بضحك   \n",
              "89           89                                  حتى هاد الكي خطار   \n",
              "90           90                                        قلتلها يمكن   \n",
              "66           66  النيابة العامة الإسبانية تطالب بسجن مدرب المنت...   \n",
              "\n",
              "                                                  Link  React  \\\n",
              "79   https://web.facebook.com/alwa3d4/posts/pfbid02...   9900   \n",
              "68   https://web.facebook.com/alwa3d4/posts/pfbid0x...   9500   \n",
              "104  https://web.facebook.com/alwa3d4/posts/pfbid0r...   9300   \n",
              "169  https://web.facebook.com/alwa3d4/posts/pfbid03...   9200   \n",
              "25   https://web.facebook.com/alwa3d4/posts/pfbid02...   8900   \n",
              "..                                                 ...    ...   \n",
              "92   https://web.facebook.com/alwa3d4/posts/pfbid02...     73   \n",
              "93   https://web.facebook.com/alwa3d4/posts/pfbid0z...     63   \n",
              "89   https://web.facebook.com/alwa3d4/posts/pfbid03...     43   \n",
              "90   https://web.facebook.com/alwa3d4/posts/pfbid02...     29   \n",
              "66   https://web.facebook.com/alwa3d4/posts/pfbid02...     28   \n",
              "\n",
              "                Date Comments  \n",
              "79               4 j      834  \n",
              "68               4 j    2,3 K  \n",
              "104  21 mars à 02:14      688  \n",
              "169  17 mars à 20:13      678  \n",
              "25               1 j      546  \n",
              "..               ...      ...  \n",
              "92   24 mars à 04:07       10  \n",
              "93   24 mars à 03:46        2  \n",
              "89               6 j        6  \n",
              "90               6 j        3  \n",
              "66               3 j        1  \n",
              "\n",
              "[262 rows x 6 columns]"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "series = pd.read_csv('series.csv')\n",
        "\n",
        "# Correctly replace commas with dots in the 'React' column\n",
        "series['React'] = series['React'].str.replace(\",\", \"\")\n",
        "\n",
        "# Remove any leading/trailing whitespace and non-breaking space before \"K\"\n",
        "series['React'] = series['React'].str.strip().str.rstrip('\\xa0')\n",
        "\n",
        "# Efficiently replace \"K\" with \"000\" and remove non-breaking space\n",
        "series['React'] = pd.to_numeric(series['React'].str.replace(\"K\", \"00\").str.replace('\\xa0', ''))\n",
        "\n",
        "# Display the corrected DataFrame\n",
        "series = series.sort_values(by='React', ascending=False)\n",
        "series"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Facebook Scraper Class\n",
        "\n",
        "This code defines a class `FacebookScraper` that utilizes the `facebook_scraper` library to extract data from Facebook posts. Here's an overview:\n",
        "\n",
        "1. Import required libraries:\n",
        "    ```python\n",
        "    import facebook_scraper as fs\n",
        "    import pandas as pd\n",
        "    from facebook_scraper import exceptions  # Import specific exceptions\n",
        "    ```\n",
        "\n",
        "2. Define the `FacebookScraper` class:\n",
        "    - Constructor:\n",
        "        - Initializes the maximum number of comments to retrieve (`MAX_COMMENTS`).\n",
        "    - Method `getPostData`:\n",
        "        - Parameters:\n",
        "            - `post_url`: URL of the Facebook post.\n",
        "        - Returns:\n",
        "            - If successful, returns a DataFrame containing post comments.\n",
        "            - If unsuccessful, prints an error message and returns `None`.\n",
        "    - Inside the method:\n",
        "        - Extracts the post ID from the provided URL.\n",
        "        - Attempts to retrieve post data using `facebook_scraper`.\n",
        "        - Handles potential errors such as missing comments or invalid URLs.\n",
        "        - If comments are found, normalizes the JSON data into a DataFrame.\n",
        "\n",
        "3. Exception handling:\n",
        "    - Catches `ValueError`, `IndexError`, and specific exceptions from `facebook_scraper`.\n",
        "    - Prints error messages and returns `None` in case of failure.\n",
        "\n",
        "This class provides a structured approach to scraping Facebook post comments and handling potential errors.\n"
      ],
      "metadata": {
        "id": "9aJkIkjeZnPA"
      },
      "id": "9aJkIkjeZnPA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a3389ae",
      "metadata": {
        "id": "5a3389ae"
      },
      "outputs": [],
      "source": [
        "import facebook_scraper as fs\n",
        "import pandas as pd\n",
        "from facebook_scraper import exceptions  # Import specific exceptions\n",
        "\n",
        "class FacebookScraper:\n",
        "    def __init__(self):\n",
        "        self.MAX_COMMENTS = 800\n",
        "\n",
        "    def getPostData(self, post_url):\n",
        "        try:\n",
        "            post_id = post_url.split(\"/\")[-1].split(\"?\")[0]  # Extract post ID\n",
        "            print(post_id)\n",
        "\n",
        "            # Attempt to get post data, handling potential errors\n",
        "            gen = fs.get_posts(post_urls=[post_id], options={\"comments\": self.MAX_COMMENTS, \"progress\": True})\n",
        "            post = next(gen)\n",
        "\n",
        "            # Handle missing 'comments_full' key\n",
        "            comments = post.get('comments_full', [])  # Use default empty list if missing\n",
        "\n",
        "            if comments:\n",
        "                df = pd.json_normalize(comments, sep='_')\n",
        "                return df\n",
        "            else:\n",
        "                print(f\"No comments found for post: {post_id}\")\n",
        "                return None  # Return None to indicate no comments\n",
        "\n",
        "        except (ValueError, IndexError, exceptions) as e:\n",
        "            print(f\"Error retrieving post data: {post_url} - {e}\")\n",
        "            return None  # Return None to signal failure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping Facebook Post Data\n",
        "\n",
        "This code snippet utilizes a Facebook scraper instance (`fss`) to scrape data from a list of Facebook posts. Here's an overview:\n",
        "\n",
        "1. Import required libraries:\n",
        "    ```python\n",
        "    import pandas as pd\n",
        "    from facebook_scraper import exceptions  # Import specific exceptions\n",
        "    ```\n",
        "\n",
        "2. Initialize a list to store all the DataFrames:\n",
        "    ```python\n",
        "    all_post_data = []\n",
        "    ```\n",
        "\n",
        "3. Create a FacebookScraper instance (`fss`):\n",
        "    ```python\n",
        "    fss = FacebookScraper()\n",
        "    ```\n",
        "\n",
        "4. Limit the number of posts to scrape (adjust as needed):\n",
        "    ```python\n",
        "    seriesPost = series[:25]  # For example, limit to the first 25 posts\n",
        "    ```\n",
        "\n",
        "5. Iterate through the posts and titles:\n",
        "    ```python\n",
        "    for p, c in zip(seriesPost['Link'], seriesPost['Title']):\n",
        "    ```\n",
        "\n",
        "6. Inside the loop:\n",
        "    - Attempt to scrape post data using the `getPostData` method from `FacebookScraper`.\n",
        "    - If post data is successfully retrieved, add the category (`Title`) to the DataFrame and append it to the `all_post_data` list.\n",
        "    - Handle potential errors and print error messages.\n",
        "\n",
        "7. Combine all DataFrames into a single DataFrame:\n",
        "    ```python\n",
        "    if all_post_data:\n",
        "        all_comments_df = pd.concat(all_post_data, ignore_index=True)\n",
        "        # Optionally save the DataFrame to a CSV file\n",
        "        # all_comments_df.to_csv('all_comments.csv', index=False)\n",
        "    else:\n",
        "        print(\"No posts were successfully scraped.\")\n",
        "    ```\n",
        "\n",
        "This code effectively scrapes data from Facebook posts, handles errors gracefully, and combines the extracted data into a single DataFrame.\n"
      ],
      "metadata": {
        "id": "Zv1Mvt8UZywY"
      },
      "id": "Zv1Mvt8UZywY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "834c64e4",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "477e25794b184536a953723a8c36d3cc",
            "6bbe2c188f21470f99dc6c7fe2b1e7c7",
            "742bc7712ca64f6aade3013fe73e6983",
            "31bbffee98354e439bf350bf0ae68f4b",
            "6e30e5f1bcdb451b994545476e972e9f",
            "c7a27a35fbc146108a1dac0d9cf18dc2",
            "43865a169fb546a48645cba536875f7b",
            "581e29ae555f4201a240d5ddf66f9c56",
            "bf8dad8898554f1d8719e4af94413415"
          ]
        },
        "id": "834c64e4",
        "outputId": "6c57388d-2adc-4b69-b23a-9d27b8c39eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pfbid022jobutRenT81bktfoSPcooCsHSB6gDhQgdvXtaq5SVMcP9FzLQXnHawGCujvHsvUl\n",
            "No comments found for post: pfbid022jobutRenT81bktfoSPcooCsHSB6gDhQgdvXtaq5SVMcP9FzLQXnHawGCujvHsvUl\n",
            "pfbid0x97YYD17YkjBF96PPzXkZFU4Y1ef1k4p5bXx3LfHGtSmaLbF68PLCRkM2yjAHdoVl\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "477e25794b184536a953723a8c36d3cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pfbid0rdEgsMk8WkUhE46WqdMb79hXphkcf5YfGF7icwUrMi7PikBKkowfKxJ3xCqdd2ful\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bbe2c188f21470f99dc6c7fe2b1e7c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pfbid036eQymCbqipSS5RywFquVzqV8d5givbiLwkSC7SqWkKx8DFSVNZN9XPaMzrknLSynl\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "742bc7712ca64f6aade3013fe73e6983",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pfbid02LpSTbuPczSTmNAjAmqVFpNYs9txg8JdG8Uu1uy5c6bN5RPvxnQtqRM3YoZtCMF8Nl\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31bbffee98354e439bf350bf0ae68f4b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pfbid0SMQh8mTQu9zmCB1R92hKVsFpR3YAirDtNQqva5GYyUH6PKWXUckME7hHHb6o3Prkl\n",
            "No comments found for post: pfbid0SMQh8mTQu9zmCB1R92hKVsFpR3YAirDtNQqva5GYyUH6PKWXUckME7hHHb6o3Prkl\n",
            "pfbid0hzHoNxR1vTCmuAMMSs8TZGYPU9BK8LBDmhfvAfoDtQE4Rec1U7iTwGDzMMUmJGFKl\n",
            "No comments found for post: pfbid0hzHoNxR1vTCmuAMMSs8TZGYPU9BK8LBDmhfvAfoDtQE4Rec1U7iTwGDzMMUmJGFKl\n",
            "pfbid02YE9KT6yBpZojfMzGXqfxKiDzeS95WyD14z5Dfp3jjkTS2kNpTbMiUAZw9KhULTAzl\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e30e5f1bcdb451b994545476e972e9f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pfbid02PHBEvnq4NSJcEoKFqFV7kzaKGgUzLJrGuKut3ScaK3vW5XzGVpBpiLkbzyptcvXYl\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7a27a35fbc146108a1dac0d9cf18dc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pfbid0jfpCnhkf7fVX9Pqh3A4SDQVuHEeWm7YRojCojgQjVkcjwAK2Bjijef2aja9jXBfkl\n",
            "No comments found for post: pfbid0jfpCnhkf7fVX9Pqh3A4SDQVuHEeWm7YRojCojgQjVkcjwAK2Bjijef2aja9jXBfkl\n",
            "pfbid02A88Jhz8qh8Qbjps9rPv1j7BHMgYd8AwRzdFkzNBe5umhRdtZde6kH2Uah1S5bRj1l\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43865a169fb546a48645cba536875f7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pfbid02ck2hR6pk41nhiK6nsmQ6tdFP2Xk2YSwimDYQa8VRJfchwuXN3ESR5u7yTp5TZCqql\n",
            "No comments found for post: pfbid02ck2hR6pk41nhiK6nsmQ6tdFP2Xk2YSwimDYQa8VRJfchwuXN3ESR5u7yTp5TZCqql\n",
            "pfbid02vptCSHfAGrZbANQRF24hb8nRPJzJPrHysgdMbAwvqQxP5T3KK3kdz2yzgoJ1G74Rl\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "581e29ae555f4201a240d5ddf66f9c56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pfbid02wcCtJPsdYgz6gnXW327wydYDrirB2EcWwTZPgPGh4GLmZBoRVSKyaPboGTv88d2fl\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf8dad8898554f1d8719e4af94413415",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pfbid027VfksptMrAQQcFNz5p2DBCTjSoeEHoJANi8on2s7V6X5dFsPLgNV3AHjxtUTN2wsl\n",
            "No comments found for post: pfbid027VfksptMrAQQcFNz5p2DBCTjSoeEHoJANi8on2s7V6X5dFsPLgNV3AHjxtUTN2wsl\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from facebook_scraper import exceptions  # Import specific exceptions\n",
        "\n",
        "# Initialize a list to store all the DataFrames\n",
        "all_post_data = []\n",
        "\n",
        "# Create a Facebook scraper instance\n",
        "fss = FacebookScraper()\n",
        "\n",
        "# Limit the number of posts to scrape (adjust as needed)\n",
        "seriesPost = series[:25]\n",
        "\n",
        "for p, c in zip(seriesPost['Link'], seriesPost['Title']):\n",
        "    try:\n",
        "        # Attempt to scrape post data\n",
        "        post_data = fss.getPostData(p)\n",
        "\n",
        "        if post_data is not None:\n",
        "            # Add category and append to list if successful\n",
        "            post_data['Title'] = c\n",
        "            all_post_data.append(post_data)\n",
        "\n",
        "    except (ValueError, IndexError, ) as e:\n",
        "        print(f\"Error scraping post: {p} - {e}\")\n",
        "\n",
        "# Combine all DataFrames into a single DataFrame (assuming compatible structures)\n",
        "if all_post_data:\n",
        "    all_comments_df = pd.concat(all_post_data, ignore_index=True)\n",
        "    # Save the DataFrame to a CSV file (optional)\n",
        "    # all_comments_df.to_csv('all_comments.csv', index=False)\n",
        "else:\n",
        "    print(\"No posts were successfully scraped.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a6da432",
      "metadata": {
        "id": "7a6da432"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2f167df",
      "metadata": {
        "id": "a2f167df"
      },
      "outputs": [],
      "source": [
        "all_comments_df= pd.concat(all_post_data, ignore_index=True)\n",
        "comments_ds = all_comments_df[['comment_text', 'comment_time', 'Title']]\n",
        "comments_ds.to_csv(\"facebook_comments.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Request to Continue Scraping or Change Page\n",
        "\n",
        "Dear Team,\n",
        "\n",
        "To continue scraping data from Facebook posts, you have two options:\n",
        "\n",
        "### Option 1: Continue from Line 25\n",
        "\n",
        "You can resume the scraping process from line 25 of the existing code snippet. This allows you to continue scraping data from the remaining posts in the provided list.\n",
        "\n",
        "### Option 2: Change the Page\n",
        "\n",
        "Alternatively, you can change the Facebook page being scraped. Simply replace the `series` DataFrame with another DataFrame containing the posts of a different Facebook page. Then, execute the code from the beginning to scrape data from the new page.\n",
        "\n",
        "Please choose the option that best fits your requirements.\n",
        "\n",
        "Best regards,\n",
        "Soufiane\n"
      ],
      "metadata": {
        "id": "aQm0Ht7zaLoK"
      },
      "id": "aQm0Ht7zaLoK"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}