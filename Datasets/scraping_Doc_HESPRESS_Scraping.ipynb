{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u9RsUlpLM-zk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf5KrEb6vrkR"
      },
      "source": [
        "# Projet de Scraping des Réseaux Sociaux\n",
        "\n",
        "Ce projet consiste à développer des outils de scraping pour extraire des données à partir de différentes plateformes de médias sociaux telles que YouTube, Twitter et Facebook. Les outils développés permettent d'extraire des commentaires, des publications et d'autres informations pertinentes à partir de ces plateformes.\n",
        "\n",
        "## Installation\n",
        "\n",
        "Pour utiliser les outils de scraping pour chaque plateforme, suivez les étapes d'installation suivantes :\n",
        "\n",
        "### Installation des Dépendances\n",
        "\n",
        "Assurez-vous d'installer les dépendances requises en exécutant la commande suivante :\n",
        "\n",
        "```bash\n",
        "pip install pandas twint facebook-scraper google-api-python-client\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TIi1jod-y9u9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TwitterScraper Class\n",
        "\n",
        "La classe `TwitterScraper` est conçue pour extraire des tweets de Twitter en utilisant l'API Twint. Elle utilise la bibliothèque `twscrape` pour interagir avec l'API Twint de manière asynchrone.\n",
        "\n",
        "## Méthodes\n",
        "\n",
        "### `__init__(self)`\n",
        "- **Description:** Initialise une instance de la classe `TwitterScraper`.\n",
        "- **Paramètres:** Aucun paramètre requis.\n",
        "\n",
        "### `gather_tweets(self, query, limit=20)`\n",
        "- **Description:** Récupère les tweets basés sur une requête donnée.\n",
        "- **Paramètres:**\n",
        "  - `query`: La requête à rechercher sur Twitter.\n",
        "  - `limit`: Le nombre maximum de tweets à récupérer (par défaut: 20).\n",
        "- **Retourne:** Un DataFrame pandas contenant les données des tweets récupérés.\n"
      ],
      "metadata": {
        "id": "u4phY9gboTdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from twscrape import API, gather\n",
        "import pandas as pd\n",
        "\n",
        "class TwitterScraper:\n",
        "    def __init__(self):\n",
        "        self.api = API()\n",
        "\n",
        "    async def gather_tweets(self, query, limit=20):\n",
        "        await self.api.pool.add_account(\"user\", \"pass\", \"mail\", \"mail_pass1\") #mail pass optionel  , we can have more than 1\n",
        "        await self.api.pool.login_all()\n",
        "\n",
        "        tweets = await gather(self.api.search(query, limit=limit))\n",
        "\n",
        "        data = []\n",
        "        for tweet in tweets:\n",
        "            tweet_data = {\n",
        "                'ID': tweet.id,\n",
        "                'Username': tweet.user.username,\n",
        "                'Content': tweet.rawContent,\n",
        "                'Date': tweet.date\n",
        "            }\n",
        "            data.append(tweet_data)\n",
        "\n",
        "            print(tweet.id, tweet.user.username, tweet.rawContent)\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        return df\n",
        "\n"
      ],
      "metadata": {
        "id": "1OFdK0xNoSKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dgWO2qwSixNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FacebookScraper Class\n",
        "\n",
        "La classe `FacebookScraper` est conçue pour extraire des commentaires de publications Facebook en utilisant la bibliothèque `facebook_scraper`.\n",
        "\n",
        "## Méthodes\n",
        "\n",
        "### `__init__(self)`\n",
        "- **Description:** Initialise une instance de la classe `FacebookScraper`.\n",
        "- **Paramètres:** Aucun paramètre requis.\n",
        "\n",
        "### `getPostData(self, post_url)`\n",
        "- **Description:** Récupère les données des commentaires d'une publication Facebook spécifique.\n",
        "- **Paramètres:**\n",
        "  - `post_url`: L'URL de la publication Facebook.\n",
        "- **Retourne:** Un DataFrame pandas contenant les données des commentaires extraits.\n"
      ],
      "metadata": {
        "id": "AOyHeI5dpswp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import facebook_scraper as fs\n",
        "import pandas as pd\n",
        "\n",
        "class FacebookScraper:\n",
        "    def __init__(self):\n",
        "        self.MAX_COMMENTS = 100\n",
        "\n",
        "    def getPostData(self, post_url):\n",
        "        post_id = post_url.split(\"/\")[-2]\n",
        "        # Assurez-vous que l'ID ne contient pas d'autres caractères\n",
        "        post_id=post_id.split(\"?\")[0]\n",
        "        gen = fs.get_posts(post_urls=[post_id], options={\"comments\": self.MAX_COMMENTS, \"progress\": True})\n",
        "        post = next(gen)\n",
        "        comments = post['comments_full']\n",
        "        df = pd.json_normalize(comments, sep='_')\n",
        "        return df\n",
        "\n"
      ],
      "metadata": {
        "id": "wDRHLGmyoSb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TasSTuTro2hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1FdOy3hTo3Jr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nma_JWh-W-IF"
      },
      "source": [
        "# YouTubeCommentScraper Class\n",
        "\n",
        "La classe `YouTubeCommentScraper` est conçue pour extraire des informations sur les vidéos et les commentaires de YouTube en utilisant l'API YouTube Data v3.\n",
        "\n",
        "## Méthodes\n",
        "\n",
        "### `__init__(self)`\n",
        "- **Description:** Initialise une instance de la classe `YouTubeCommentScraper`.\n",
        "- **Paramètres:**\n",
        "  - `api_key`: La clé API YouTube pour l'authentification.\n",
        "  \n",
        "### `get_video_info(self, video_id)`\n",
        "- **Description:** Récupère les informations sur une vidéo spécifique.\n",
        "- **Paramètres:**\n",
        "  - `video_id`: L'identifiant unique de la vidéo sur YouTube.\n",
        "\n",
        "### `get_comments_df_for_video(self, video_url, max_results=50)`\n",
        "- **Description:** Récupère les commentaires d'une vidéo et les stocke dans un DataFrame pandas.\n",
        "- **Paramètres:**\n",
        "  - `video_url`: L'URL de la vidéo YouTube.\n",
        "  - `max_results`: Le nombre maximum de commentaires à récupérer (par défaut: 50).\n",
        "\n",
        "### `get_comments_info_for_topic(self, topic, max_results=10)`\n",
        "- **Description:** Récupère les commentaires associés à un sujet donné.\n",
        "- **Paramètres:**\n",
        "  - `topic`: Le sujet ou le mot-clé à rechercher sur YouTube.\n",
        "  - `max_results`: Le nombre maximum de vidéos et de commentaires à récupérer pour ce sujet (par défaut: 10).\n",
        "\n",
        "### `get_comments_df_for_topic(self, topic, max_results=10)`\n",
        "- **Description:** Récupère les commentaires associés à un sujet donné et les stocke dans un DataFrame pandas.\n",
        "- **Paramètres:**\n",
        "  - `topic`: Le sujet ou le mot-clé à rechercher sur YouTube.\n",
        "  - `max_results`: Le nombre maximum de vidéos et de commentaires à récupérer pour ce sujet (par défaut: 10).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwFnJsE6vjf8"
      },
      "outputs": [],
      "source": [
        "import googleapiclient.discovery\n",
        "import pandas as pd\n",
        "import re\n",
        "class YouTubeCommentScraper:\n",
        "    def __init__(self, ):\n",
        "       # self.api_key = 'API key'\n",
        "        self.youtube = googleapiclient.discovery.build('youtube', 'v3', developerKey='ApiKey')\n",
        "\n",
        "    def get_video_info(self, video_id):\n",
        "        request = self.youtube.videos().list(part='snippet', id=video_id)\n",
        "        response = request.execute()\n",
        "\n",
        "        if 'items' in response:\n",
        "            video = response['items'][0]\n",
        "            title = video['snippet']['title']\n",
        "            description = video['snippet']['description']\n",
        "            print(f'Title: {title}\\nDescription: {description}')\n",
        "\n",
        "    def get_comments_df_for_video(self, video_url,max_results=50):\n",
        "        # Extraire l'ID de la vidéo à partir de l'URL\n",
        "        video_id_match = re.search(r'(?:youtu\\.be/|youtube\\.com/watch\\?v=|youtube\\.com/embed/|youtube\\.com/v/|youtube\\.com/e/|youtube\\.com/user/[^/]+/u/|v=)([^\"&?/ ]{11})', video_url)\n",
        "        if video_id_match:\n",
        "            video_id = video_id_match.group(1)\n",
        "        else:\n",
        "            raise ValueError(\"L'URL de la vidéo YouTube est invalide.\")\n",
        "\n",
        "        comments_request = self.youtube.commentThreads().list(\n",
        "                part='snippet,replies',\n",
        "                videoId=video_id,\n",
        "                textFormat='plainText',\n",
        "                maxResults=max_results\n",
        "            )\n",
        "        comments_response = comments_request.execute()\n",
        "        comments_info = []\n",
        "        for item in comments_response['items']:\n",
        "                snippet = item['snippet']['topLevelComment']['snippet']\n",
        "                comments_info.append({\n",
        "                    'Video Title': snippet.get('videoOwnerChannelInfo', {}).get('videoOwnerChannelTitle', ''),\n",
        "                    'Comment Date': snippet.get('publishedAt', ''),\n",
        "                    'Comment': snippet.get('textDisplay', ''),\n",
        "                    'Likes': snippet.get('likeCount', 0),\n",
        "                    'Dislikes': snippet.get('dislikeCount', 0)\n",
        "                })\n",
        "\n",
        "        return pd.DataFrame(comments_info)\n",
        "\n",
        "\n",
        "    def get_comments_info_for_topic(self, topic, max_results=10):\n",
        "        search_request = self.youtube.search().list(\n",
        "            q=topic,\n",
        "            part='id',\n",
        "            type='video',\n",
        "            maxResults=max_results\n",
        "        )\n",
        "        search_response = search_request.execute()\n",
        "\n",
        "        video_ids = [item['id']['videoId'] for item in search_response['items']]\n",
        "\n",
        "        comments_info = []\n",
        "        for video_id in video_ids:\n",
        "            comments_request = self.youtube.commentThreads().list(\n",
        "                part='snippet,replies',\n",
        "                videoId=video_id,\n",
        "                textFormat='plainText',\n",
        "                maxResults=max_results\n",
        "            )\n",
        "            comments_response = comments_request.execute()\n",
        "\n",
        "            for item in comments_response['items']:\n",
        "                snippet = item['snippet']['topLevelComment']['snippet']\n",
        "                comments_info.append({\n",
        "                    'Video Title': snippet.get('videoOwnerChannelInfo', {}).get('videoOwnerChannelTitle', ''),\n",
        "                    'Comment Date': snippet.get('publishedAt', ''),\n",
        "                    'Comment': snippet.get('textDisplay', ''),\n",
        "                    'Likes': snippet.get('likeCount', 0),\n",
        "                    'Dislikes': snippet.get('dislikeCount', 0)\n",
        "                })\n",
        "\n",
        "        return comments_info\n",
        "\n",
        "    def get_comments_df_for_topic(self, topic, max_results=10):\n",
        "        comments_info = self.get_comments_info_for_topic(topic, max_results)\n",
        "        return pd.DataFrame(comments_info)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Méthodes de Nettoyage des Commentaires\n",
        "\n",
        "### `clean_youtube_comments(self, df)`\n",
        "- **Description:** Nettoie les commentaires extraits de YouTube en ne gardant que les commentaires en arabe et en supprimant les hashtags.\n",
        "- **Paramètres:**\n",
        "  - `df`: Un DataFrame pandas contenant les données des commentaires de YouTube.\n",
        "- **Retourne:** Un DataFrame pandas contenant les commentaires nettoyés et leur date correspondante.\n",
        "\n",
        "### `clean_facebook_comments(self, df)`\n",
        "- **Description:** Nettoie les commentaires extraits de Facebook en ne gardant que les commentaires en arabe.\n",
        "- **Paramètres:**\n",
        "  - `df`: Un DataFrame pandas contenant les données des commentaires de Facebook.\n",
        "- **Retourne:** Un DataFrame pandas contenant les commentaires nettoyés et leur date correspondante.\n",
        "\n"
      ],
      "metadata": {
        "id": "CoYN4QV6o1Yt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "gJr_9dXGpJ05",
        "outputId": "9f556d03-ec67-4950-a485-cfdba9ddd14d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "86400"
            ]
          },
          "execution_count": 0,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean_youtube_comments(self, df):\n",
        "    cleaned_comments = []\n",
        "    arabic_letters_pattern = re.compile('[\\u0600-\\u06FF\\s]+')  # Expression régulière pour les lettres en arabe\n",
        "    french_letters_pattern = re.compile('[a-zA-Z]+')  # Expression régulière pour les lettres en français\n",
        "\n",
        "    cleaned_data = {'Comment': [], 'Date': []}\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # Supprimer les hashtags et ne garder que les lettres en arabe\n",
        "        comment_text = re.sub(r'#\\w+', '', row['Comment'])\n",
        "        arabic_text = ''.join(arabic_letters_pattern.findall(comment_text))\n",
        "\n",
        "        # Ajouter le commentaire nettoyé uniquement s'il contient des lettres en arabe et n'est pas vide\n",
        "        if arabic_text and len(arabic_text.strip()) > 0:\n",
        "            # Supprimer les commentaires avec plus de 10 lettres en français\n",
        "            french_text = ''.join(french_letters_pattern.findall(comment_text))\n",
        "            if len(french_text) <= 10:\n",
        "                cleaned_data['Comment'].append(arabic_text)\n",
        "                cleaned_data['Date'].append(row['Comment Date'])\n",
        "\n",
        "    cleaned_df = pd.DataFrame(cleaned_data)\n",
        "    return cleaned_df\n",
        "\n",
        "def clean_facebook_comments(self,df):\n",
        "    cleaned_data = {'Comment': [], 'Date': []}\n",
        "    arabic_letters_pattern = re.compile('[\\u0600-\\u06FF\\s]+')  # Expression régulière pour les lettres en arabe\n",
        "    french_letters_pattern = re.compile('[a-zA-Z]+')  # Expression régulière pour les lettres en français\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        # Ajouter le commentaire nettoyé uniquement s'il contient des lettres en arabe et n'est pas vide\n",
        "        comment_text = row['comment_text']\n",
        "        arabic_text = ''.join(arabic_letters_pattern.findall(comment_text))\n",
        "\n",
        "        if arabic_text and len(arabic_text.strip()) > 0:\n",
        "            # Supprimer les commentaires avec plus de 10 lettres en français\n",
        "            french_text = ''.join(french_letters_pattern.findall(comment_text))\n",
        "            if len(french_text) <= 10:\n",
        "                cleaned_data['Comment'].append(arabic_text)\n",
        "                cleaned_data['Date'].append(row['comment_time'])\n",
        "\n",
        "    cleaned_df = pd.DataFrame(cleaned_data)\n",
        "    return cleaned_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fhs6GZ4qFMx"
      },
      "source": [
        "#### Utilisation de YouTubeScraper pour Extraire des Commentaires d'une Vidéo YouTube\n",
        "\n",
        "```python\n",
        "from youtube_scraper import YouTubeScraper\n",
        "\n",
        "scraper = YouTubeScraper()\n",
        "comments_df = scraper.get_comments_df_for_video(\"URL_de_la_vidéo\")\n",
        "print(comments_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Exemple d'Utilisation de TwitterScraper\n",
        "\n",
        "#### Utilisation de TwitterScraper pour Extraire des Tweets basés sur une Requête\n",
        "\n",
        "```python\n",
        "from twitter_scraper import TwitterScraper\n",
        "\n",
        "scraper = TwitterScraper()\n",
        "tweets_df = await scraper.gather_tweets(\"python\", limit=50)\n",
        "print(tweets_df)\n"
      ],
      "metadata": {
        "id": "wPeISBTZrPEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Exemple d'Utilisation de FacebookScraper\n",
        "\n",
        "#### Utilisation de FacebookScraper pour Extraire des Commentaires d'une Publication Facebook\n",
        "\n",
        "```python\n",
        "from facebook_scraper import FacebookScraper\n",
        "\n",
        "scraper = FacebookScraper()\n",
        "post_url = \"URL_de_la_publication\"\n",
        "comments_df = scraper.getPostData(post_url)\n",
        "print(comments_df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0e9TEhCcrY3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hespress comments scraping\n",
        "# Outil de Scraping de Sites Web avec Selenium\n",
        "\n",
        "Cet outil est conçu pour extraire des données à partir de sites web en utilisant Selenium, BeautifulSoup et d'autres bibliothèques Python.\n",
        "\n",
        "## Description\n",
        "\n",
        "L'outil utilise Selenium pour automatiser l'interaction avec les sites web, notamment pour charger les pages, cliquer sur des éléments et récupérer le contenu. BeautifulSoup est utilisé pour analyser le HTML des pages web et extraire les données souhaitées.\n",
        "\n",
        "## Dépendances\n",
        "\n",
        "Assurez-vous d'installer les dépendances requises en exécutant la commande suivante :\n",
        "\n",
        "```bash\n",
        "pip install selenium beautifulsoup4\n",
        "```\n"
      ],
      "metadata": {
        "id": "E-A-2ORn-K6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "bLrL0684-LQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fonction de Scraping des Articles Hespress\n",
        "\n",
        "Cette fonction permet de rechercher des articles sur le site Hespress en utilisant des mots-clés spécifiés et de récupérer les titres, liens et dates des articles correspondants.\n",
        "\n",
        "### Paramètres\n",
        "\n",
        "- `mots` : Les mots-clés à rechercher sur Hespress.\n",
        "- `rep` (optionnel) : Le nombre de fois à répéter le défilement de la page pour charger plus d'articles (par défaut: 20).\n"
      ],
      "metadata": {
        "id": "p3XhNtuF-L-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hespress_articles(mots,rep=20):\n",
        "    driver = webdriver.Chrome('C:/Users/sejja/chromedriver')\n",
        "    link=\"https://www.hespress.com/?s=\"+mots\n",
        "    driver.get(link)\n",
        "\n",
        "    # determine the rep enough to scroll all the page\n",
        "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "    for i in range(rep):\n",
        "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
        "        time.sleep(1)\n",
        "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        if new_height == last_height:\n",
        "            break\n",
        "        last_height = new_height  # Update last_height\n",
        "\n",
        "    src = driver.page_source\n",
        "    soup = BeautifulSoup(src, 'lxml')\n",
        "\n",
        "    uls = soup.find('div', {'class': 'search_results row'})\n",
        "    data=[]\n",
        "    for div in uls.findAll('div', {'class': 'col-12 col-sm-6 col-md-4 col-xl-3'}):  # Iterate over the ResultSet directly\n",
        "        try:\n",
        "            title = div.find('h3', {'class': 'card-title'}).text.strip()\n",
        "            link = div.find('a', {'class': 'stretched-link'}).get('href')\n",
        "            date = div.find('small', {'class': 'text-muted time'}).text.strip()\n",
        "            data.append({'Title': title, 'Link': link, 'Date': date})\n",
        "        except AttributeError:\n",
        "            pass\n",
        "    for div in uls.findAll('div', {'class': 'col-12 col-sm-6 col-md-6 col-xl-4 px-2'}):  # Iterate over the ResultSet directly\n",
        "        try:\n",
        "            title = div.find('h3', {'class': 'card-title'}).text.strip()\n",
        "            link = div.find('a', {'class': 'stretched-link'}).get('href')\n",
        "            date = div.find('small', {'class': 'text-muted time'}).text.strip()\n",
        "            data.append({'Title': title, 'Link': link, 'Date': date})\n",
        "        except AttributeError:\n",
        "            pass\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    df.shape\n",
        "\n",
        "    driver.close()\n",
        "    driver.quit()\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "r7KyX4Xw-MNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fonction d'Extraction de Données depuis une URL\n",
        "\n",
        "Cette fonction extrait des données à partir d'une URL donnée en utilisant Selenium et BeautifulSoup. Elle récupère le titre de l'article, sa date de publication, les balises associées, ainsi que les commentaires associés à l'article.\n",
        "\n",
        "### Paramètres\n",
        "\n",
        "- `url` (str): L'URL de la page web à scraper.\n",
        "\n",
        "### Retour\n",
        "\n",
        "- dict: Un dictionnaire contenant les données extraites de la page.\n",
        "  - `Date` (str): La date de publication de l'article.\n",
        "  - `Titre` (str): Le titre de l'article.\n",
        "  - `Tags` (str): Les balises associées à l'article (séparées par des virgules).\n",
        "  - `Comments` (list): Une liste contenant les commentaires associés à l'article, chaque commentaire étant représenté sous forme de dictionnaire avec les clés suivantes :\n",
        "    - `comment_date` (str): La date du commentaire.\n",
        "    - `comment_content` (str): Le contenu du commentaire.\n",
        "    - `comment_react` (str): La réaction associée au commentaire.\n"
      ],
      "metadata": {
        "id": "yUY_kGCr-7RR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_data(url):\n",
        "    \"\"\"Extracts data from a given URL using Selenium and BeautifulSoup.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the webpage to scrape.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the extracted data,\n",
        "              or None if an error occurs.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        driver = webdriver.Chrome('C:/Users/sejja/chromedriver')  # Assuming valid path\n",
        "        driver.get(url)\n",
        "        time.sleep(3)\n",
        "\n",
        "        src = driver.page_source\n",
        "        soup = BeautifulSoup(src, 'lxml')\n",
        "\n",
        "        titre = soup.find('h1', {'class': 'post-title'})\n",
        "        if titre:\n",
        "            titre = titre.get_text().strip()\n",
        "        else:\n",
        "            titre = \"not available\"\n",
        "\n",
        "        date = soup.find('span', {'class': 'date-post'})\n",
        "        if date:\n",
        "            date = date.get_text().strip()\n",
        "        else:\n",
        "            date = \"not available\"\n",
        "\n",
        "        sections = soup.find('section', {'class': 'box-tags'})\n",
        "        tags = \"\"\n",
        "        if sections:\n",
        "            for section in sections.findAll('a'):\n",
        "                if section == sections.findAll('a')[0]:\n",
        "                    tags = section.get_text().strip()\n",
        "                else:\n",
        "                    tags += \",\"\n",
        "                    tags += section.get_text().strip()\n",
        "\n",
        "        comments_area = soup.find('ul', {'class': 'comment-list hide-comments'})\n",
        "        comments = []\n",
        "        if comments_area:\n",
        "            for comment in comments_area.findAll('li', {'class': 'comment even thread-even depth-1 not-reply'}):\n",
        "                comment_date = comment.find('div', {'class': 'comment-date'})\n",
        "                comment_content = comment.find('div', {'class': 'comment-text'})\n",
        "                comment_react = comment.find('span', {'class': 'comment-recat-number'})\n",
        "                if comment_date and comment_content and comment_react:\n",
        "                    comments.append({\n",
        "                        \"comment_date\": comment_date.get_text(),\n",
        "                        \"comment_content\": comment_content.get_text(),\n",
        "                        \"comment_react\": comment_react.get_text()\n",
        "                    })\n",
        "\n",
        "        return {'Date': date, 'Titre': titre, 'Tags': tags, 'Comments': comments}\n",
        "    except (NoSuchElementException, TimeoutException) as e:\n",
        "        print(f\"Error encountered while processing {url}: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        driver.quit()  # Ensure closing the browser even on exceptions\n",
        "\n"
      ],
      "metadata": {
        "id": "uv-dshJ4-7kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cet exemple illustre comment utiliser la fonction extract_data pour extraire les données à partir d'une URL spécifiée. Les données extraites sont affichées si aucune erreur ne s'est produite."
      ],
      "metadata": {
        "id": "PCl5BktN_Cma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = pd.DataFrame(columns=['Date', 'Titre', 'Tags', 'Comments'])\n",
        "\n",
        "for d in hespress_articles(\"رمضان\",20)['Link']:\n",
        "    extracted_data = extract_data(d)\n",
        "    if extracted_data:\n",
        "        result_df = result_df.append(extracted_data, ignore_index=True)\n",
        "result_df"
      ],
      "metadata": {
        "id": "Z5Hwd2v4_B8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ces exemples illustrent comment utiliser chaque classe de scraping pour extraire des données à partir des plateformes YouTube, Twitter et Facebook. Assurez-vous de remplacer `\"URL_de_la_vidéo\"`, `\"python\"` et `\"URL_de_la_publication\"` par les URL et requêtes appropriées pour votre utilisation spécifique.\n"
      ],
      "metadata": {
        "id": "sng6NHrMrhdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusions des Ressources et Besoins en Développement\n",
        "\n",
        "1. **Facebook**:\n",
        "   - Les ressources actuelles permettent le scraping par ID de publication, mais nous souhaitons développer une méthode pour scraper plusieurs publications à partir d'une seule page.\n",
        "   - Besoin de développer une méthode pour extraire les commentaires de plusieurs publications à partir d'une page Facebook.\n",
        "\n",
        "2. **YouTube**:\n",
        "   - Des problèmes surviennent lors du scraping par mot-clé.\n",
        "   - Besoin de gérer les exceptions pour surmonter les problèmes de scraping sur YouTube.\n",
        "\n",
        "3. **Twitter**:\n",
        "   - Besoin de développer davantage la méthode de scraping, y compris l'intégration avec Flask ou un serveur pour gérer les requêtes.\n",
        "   - Nécessité d'améliorer les fonctionnalités et la robustesse de la méthode de scraping pour Twitter.\n",
        "\n",
        "4. **Autres Méthodes**:\n",
        "   - Besoin d'explorer et de développer d'autres méthodes de scraping pour collecter des données de manière efficace et fiable à partir de différentes plateformes de médias sociaux.\n",
        "   - Recherche de méthodes plus efficaces et fiables pour collecter des données sur les autres plateformes sociales.\n",
        "\n",
        "En résumé, pour Facebook, nous avons besoin de développer une méthode pour scraper plusieurs publications à partir d'une page. Pour YouTube, il est nécessaire de gérer les exceptions lors du scraping par mot-clé. Pour Twitter, nous devons améliorer la méthode de scraping et envisager une intégration avec Flask ou un serveur. De plus, nous devons explorer d'autres méthodes pour améliorer l'efficacité du scraping sur toutes les plateformes de médias sociaux."
      ],
      "metadata": {
        "id": "XmodyAEhy_-E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mgboxXKFrYQK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}